<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning">
  <meta name="keywords" content="Diffusion LLM, Reinforcement Learning, dLLM, diffu-GRPO">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <style>
    body {
      font-family: 'Noto Sans', sans-serif;
      line-height: 1.6;
    }
    .hero {
      background-color: #f5f5f5;
    }
    .publication-title {
      font-family: 'Google Sans', sans-serif;
      font-weight: bold;
    }
    .section-title {
      margin-top: 2rem;
      margin-bottom: 1.5rem;
    }
    .content p {
      margin-bottom: 1.2rem;
    }
    .blog-section {
      margin-bottom: 3rem;
    }
    .highlight-box {
      background-color: #f8f9fa;
      padding: 1.5rem;
      border-radius: 8px;
      margin-bottom: 2rem;
      box-shadow: 0 2px 5px rgba(0,0,0,0.05);
    }
    .image-caption {
      font-size: 0.9rem;
      color: #666;
      text-align: center;
      margin-top: 0.5rem;
    }
    .key-point {
      font-weight: bold;
      color: #3273dc;
    }
    .equation-placeholder {
      background-color: #eef6ff;
      padding: 1rem;
      border-radius: 4px;
      text-align: center;
      margin: 1rem 0;
      font-style: italic;
    }
    .equal-contribution {
      font-size: 0.85rem;
      font-style: italic;
      margin-top: 0.5rem;
    }
    .figure-container {
      margin: 2rem 0;
      text-align: center;
    }
    .figure-container img {
      border-radius: 6px;
      box-shadow: 0 3px 10px rgba(0,0,0,0.1);
    }
  </style>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://siyan-zhao.github.io">Siyan Zhao*</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://devaansh100.github.io">Devaansh Gupta*</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://enosair.github.io">Qinqing Zheng</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://aditya-grover.github.io">Aditya Grover</a><sup>1</sup>
            </span>
          </div>

          <div class="equal-contribution">* Equal Contribution</div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>UCLA</span>
            <span class="author-block"><sup>2</sup>Meta AI</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/dllm-reasoning/d1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Main content -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        
        <!-- Abstract -->
        <div class="blog-section">
          <h2 class="title is-3 has-text-centered section-title">Scaling Reasoning in Diffusion LLMs via RL</h2>
          <div class="content has-text-justified">
            <div class="highlight-box">
              <p>
                Recent large language models (LLMs) have demonstrated strong reasoning capabilities that benefit from online reinforcement learning (RL). These capabilities have primarily been demonstrated within the left-to-right autoregressive (AR) generation paradigm. In contrast, non-autoregressive paradigms based on diffusion generate text in a coarse-to-fine manner. Although recent diffusion-based large language models (dLLMs) have achieved competitive language modeling performance compared to their AR counterparts, it remains unclear if dLLMs can also leverage recent advances in LLM reasoning.
              </p>
              <p>
                To this end, we propose <span class="key-point">d1</span>, a framework to adapt pre-trained masked dLLMs into reasoning models via a combination of supervised finetuning (SFT) and RL. Specifically, we develop and extend techniques to improve reasoning in pretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge and instill self-improvement behavior directly from existing datasets, and (b) we introduce a novel critic-free, policy-gradient based RL algorithm called <span class="key-point">diffu-GRPO</span> .
              </p>
              <!-- <p>
                Through empirical studies, we investigate the performance of different post-training recipes on multiple mathematical and logical reasoning benchmarks. We find that d1 yields the best performance and significantly improves performance of a state-of-the-art dLLM.
              </p> -->
            </div>
          </div>
        </div>

        <!-- Key Results -->
        <div class="blog-section">
          <!-- <h2 class="title is-3 has-text-centered section-title">Performance Improvements</h2> -->
          <div class="content has-text-justified">
            <p>Through empirical studies on multiple mathematical and logical reasoning benchmarks, we find that d1 yields the best performance and significantly improves the capabilities of state-of-the-art dLLMs.</p>
            <div class="figure-container">
              <img src="./static/images/pull_fig.png" alt="Main results" style="max-width: 80%; display: block; margin: 0 auto;">
              <p class="image-caption">Across four math and logical reasoning tasks, d1-LLaDA, which undergoes SFT followed by our proposed diffu-GRPO, consistently outperforms the base LLaDA-8B-Instruct model.</p>
            </div>
          </div>
        </div>

        <!-- SOTA Comparison -->
        <div class="blog-section">
          <h2 class="title is-3 has-text-centered section-title">Comparison with Similar-Sized Models</h2>
          <div class="content has-text-justified">
            <div class="figure-container">
              <img src="./static/images/sota.png" alt="SOTA comparison" style="max-width: 80%; display: block; margin: 0 auto;">
              <p class="image-caption">d1-LLaDA achieves the highest GSM8K score and competitive MATH500 performance compared to recent leading dLLMs and similar-sized AR LLMs.</p>
            </div>
          </div>
        </div>

        <!-- Two-Stage Training Framework -->
        <div class="blog-section">
          <h2 class="title is-3 has-text-centered section-title">d1: Our Two-Stage Training Framework</h2>
          <div class="content has-text-justified">
            <p>We propose d1, a two-stage framework to enhance reasoning capabilities in masked dLLMs. Adapting RL algorithms to masked dLLMs poses unique challenges because existing successful approaches for AR models rely on estimating policy distributions through computing log-probabilities of generated sequences, which cannot be directly applied to dLLMs. We need to efficiently compute the per-token and the sequence log-probability of dLLMs' completion o.</p>
            
            <p>Autoregressive (AR) models, such as Transformers, directly model the per-token log-probabilities, and the sequence-level log-probability of o can be easily computed through the chain rule using one forward pass. Unlike AR models, dLLMs do not adhere to sequential factorization of the sequence log-probability. Meanwhile, the per-token log-probability are also costly to compute since the decoding process invokes the unmasking predictor multiple times.</p>
            
            <p>As the first step, we propose an efficient log-probability estimator in our approach to Efficient Log Probability Estimation for Masked dLLMs. For sequence log-probability, we use a mean-field approximation that decomposes it into a product of independent per-token log-probabilities. For per-token log-probability, we introduce an estimation method that only calls fθ once.</p>
            
            <p>Mean-Field Approximation of Sequence Log Probability. As opposed to AR models, dLLMs treat the token sequence as a whole and therefore its sequence-level log-probability lacks the AR decomposition. To efficiently estimate it, we use a simple mean-field decomposition that approximates log πθ (o|q) by ∑|o|k=1 log πθ (ok|q). The per-token log-probability estimation is introduced below.</p>
            
            <p>One-Step Per-Token Log Probability Estimation with Prompt Masking. Let ⊕ denote the concatenation operator. Given a prompt q, the decoding process starts from an initial sequence q ⊕ mask ⊕ . . . ⊕ mask (up to a preset length). To compute the log-probability of o, we perturb q where every token is randomly masked out with probability pmask, resulting in a new prompt q′. We then do one-step unmasking to obtain log fθ (ok|q′ ⊕ mask . . . ⊕ mask) and use it as an estimation of log πθ (ok|q), 1 ≤ k ≤ |o|. We discuss the motivation of using a masked prompt q′ in the next section.</p>
            
            <p>We note that LLaDA uses a Monte Carlo type of approximation to estimate the log-probabilities, where the MC sample size is 128. This estimator is inefficient for online RL, since it creates a large computational graph with hundreds of forward passes, resulting in inefficient policy optimization and excessive memory usage.</p>
            
            <h3 class="title is-4">Stage 1: Masked SFT on High-Quality Reasoning Traces</h3>
            <div class="figure-container">
              <img src="./static/images/algorithm_sft.png" alt="SFT algorithm" style="max-width: 70%; display: block; margin: 0 auto;">
              <p class="image-caption">We perform SFT on s1k, a curated dataset consisting of 1000 high-quality reasoning questions. The reasoning traces exhibit detailed step-by-step problem-solving processes, including verification of intermediate results and backtracking when encountering errors.</p>
            </div>
            
            <h3 class="title is-4 mt-5">Stage 2: Efficient Policy Gradient Algorithm for dLLMs - diffu-GRPO</h3>
            <div class="figure-container">
              <img src="./static/images/logprob.png" alt="Log probability estimation" style="max-width: 80%; display: block; margin: 0 auto;">
              <p class="image-caption">Estimating log-probabilities in dLLMs requires innovative approaches since they lack the natural sequential factorization of autoregressive models.</p>
            </div>
            
            <p>Our solution: A novel approach using random prompt masking for efficient policy optimization:</p>
            
            <div class="figure-container">
              <img src="./static/images/algobox.png" alt="diffu-GRPO algorithm" style="max-width: 70%; display: block; margin: 0 auto;">
              <p class="image-caption">Our novel diffu-GRPO algorithm leverages random prompt masking for efficient log-probability estimation, which serves as a form of regularization for policy optimization.</p>
            </div>
          </div>
        </div>

        <!-- Random Masking Benefits -->
        <div class="blog-section">
          <h2 class="title is-3 has-text-centered section-title">Benefits of Random Masking</h2>
          <div class="content has-text-justified">
            <div class="figure-container">
              <img src="./static/images/mu.png" alt="Random masking efficiency" style="max-width: 80%; display: block; margin: 0 auto;">
              <p class="image-caption">Random masking consistently outperforms fixed masking and allows scaling μ (gradient updates per batch) to much higher values while maintaining or improving performance, facilitating faster convergence of RL training.</p>
            </div>
            
            <p>Our random masking approach serves as a form of regularization for policy optimization, allowing more gradient updates per batch and thereby reducing the number of online generations needed for RL training - significantly lowering computational costs.</p>
          </div>
        </div>

        <!-- Qualitative Examples -->
        <div class="blog-section">
          <h2 class="title is-3 has-text-centered section-title">"Aha Moments" in Reasoning</h2>
          <div class="content has-text-justified">
            <div class="figure-container">
              <img src="./static/images/qualitative_example_1.png" alt="Qualitative example" style="max-width: 80%; display: block; margin: 0 auto;">
              <p class="image-caption">SFT and d1-LLaDA models show self-verification and self-correction behaviors ("aha moments") in their reasoning traces.</p>
            </div>
            <p>The models trained with SFT show self-verification and self-correction behaviors in their reasoning traces, where they can recognize errors in their initial reasoning paths, backtrack, and correct to arrive at the answer.</p>
          </div>
        </div>

        <!-- BibTeX -->
        <div class="blog-section">
          <h2 class="title is-3 has-text-centered section-title">Citation</h2>
          <div class="content has-text-centered">
            <pre style="text-align: left; background-color: #f5f5f5; padding: 1rem; border-radius: 4px;"><code>@inproceedings{zhaoandgupta2025d1,
    title={d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning},
    author={Zhao, Siyan and Gupta, Devaansh and Zheng, Qinqing and Grover, Aditya},
    booktitle={preprint},
    year={2025}
}</code></pre>
          </div>
        </div>
        
      </div>
    </div>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://github.com/dllm-reasoning/d1">
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content has-text-centered">
          <p>
            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script defer src="./static/js/fontawesome.all.min.js"></script>
<script src="./static/js/bulma-carousel.min.js"></script>
<script src="./static/js/bulma-slider.min.js"></script>
<script src="./static/js/index.js"></script>

</body>
</html>